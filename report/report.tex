\def\year{2018}\relax
%Taken from file: formatting-instruction.tex
\documentclass[letterpaper]{article} %DO NOT CHANGE THIS
\usepackage{aaai18}  %Required
\usepackage{times}  %Required
\usepackage{helvet}  %Required
\usepackage{courier}  %Required
\usepackage{url}  %Required
\usepackage{graphicx}  %Required
\frenchspacing  %Required
\setlength{\pdfpagewidth}{8.5in}  %Required
\setlength{\pdfpageheight}{11in}  %Required
%PDF Info Is Required:
%   \pdfinfo{
% /Title (Removing Viruses with Machine Learning in Dr.~Mario)
% /Author (Ryan Gately and JJ Brown)}

\usepackage{natbib}
\usepackage{url}

\author{Ryan Gately and JJ Brown}
\date{\today}
\title{Removing Viruses with Machine Learning in Dr.~Mario}

\bibliographystyle{aaai}

\renewcommand{\citet}[1]{\citeauthor{#1}~\shortcite{#1}}
\renewcommand{\citep}{\cite}
\renewcommand{\citealp}[1]{\citeauthor{#1}~\citeyear{#1}}

\begin{document}

\maketitle

\begin{abstract}
We trained machine learning agents that manipulate an NES controller to play Dr.~Mario by~\cite{drmario90}, using the FCEUX emulator by~\cite{fceux18}.
Both Q-learning and SARSA learning algorithms showed some advantage over a random controller.
We compared a SARSA agent that viewed the local ``neighborhood'' around the player's capsule and controlled the controller directly to a Q-learning agent that viewed the entire topmost layer of viruses and executed high-level actions in the game. Ultimately, while we believe the local ``neighborhood'' state space may allow for the highest theoretical quality of performance, the Q-learning agent with a high-level controller yielded the best results, with the fastest rate of learning and the highest average performance.
\end{abstract}

\section{Introduction}
Video games are an interesting application for developing machine learning agents beacuse the strictly-defined playing space provides a clean, simple environment for learning. Furthermore, they are also interesting such that they challenge the player to develop an effective strategy. We researchers, then, seek to determine whether a machine learning agent, using a simple learning algorithm, can effectively develop such strategies as to ``beat'' those games on par with a human, or if the game requires such skill as to mandate a human player.

We selected the classic video game ``Dr.~Mario'' by~\cite{drmario90} as the best candidate to develop a machine learning agent for, because we believed the simple yet challenging puzzle of dropping colored capsules would be viable for a machine learning agent to solve, yet difficult enough to force the agent to develop an effective strategy for winning.


\section{Problem Definition and Approaches}
\subsection{Task Definition}
We wanted to make machine learning agents that could play the classic video game ``Dr.~Mario'' for the Nintendo Entertainment System (NES). Therefore, we decided to use the FCEUX emulator, since it presented the most convenient interface between agent code and game actions. First, it has built in support for scripting with Lua​, which provides the agent with a way to execute actions and read the game state. Specifically, the game memory is polled periodically to retrieve values for the score, as well as location and type of blocks​ in the game area. Further, there is no hidden information that a human player would not see​: the agent only has access to information that would be displayed visually to a human. Finally, FCEUX provides a convenient way for Lua scripts to send commands to the emulator to emulate pressing a button on the NES controller. For these reasons, we decided to implement our agents in Lua scripts controlling the FCEUX emulator.

In Dr.~Mario, the player controls capsules with a color at each end that slowly fall down the screen, and tries to place the colored ends on ``virus'' blocks of the same color. If four blocks of the same color touch in a straight line, they are removed, and the player is awarded 200 points for any virus blocks removed in this way. Once the player removes all of the viruses on the screen, they complete the level and move to the next one. We wanted to train machine learning agents to complete at least one level, and outperform a ``random'' controller that presses a random button every frame.

\subsection{Algorithm Definition}
We developed two machine learning agents which learn using Q-learning and using SARSA. Each associates a value \(Q(s, a)\) with each possible state/action pair, and updates those values as it is rewarded or punished for taking an action \(a\) in a state \(s\).

For Q-learning, the value is updated after performing action \(a\) in state \(s\) to arrive in state \(s'\), yielding reward \(r\), with the following algorithm:
\begin{equation}
Q(s, a) \leftarrow Q(s, a) + \alpha * (r + \gamma * {\max}_{a'} Q(s', a') - Q(s, a))
\end{equation}

And similarly, SARSA updates values with the following algorithm:
\begin{equation}
Q(s, a) \leftarrow Q(s, a) + \alpha * (r + \gamma * Q(s', a') - Q(s, a))
\end{equation}
where the agent decides to perform action \(a'\) immediately upon arriving in state \(s'\), according to the current policy.

\section{Experimental Evaluation}
\subsection{Methodology}


\subsection{Results}


\subsection{Discussion}

\section{Related Work}

\section{Future Work}
In the future, we think we could improve the efficacy of the learning algorithms by rewarding agents for accomplishing sub-goals as discussed in~\cite{banzas15} and~\cite{bhonker17}, such as rewarding the agent for matching smaller numbers of matching colors or for matching a series of only capsule blocks (which the game does not reward), rather than only rewarding it for making matches that include colored virus blocks.

\section{Conclusion}

\newpage
\section{Bibliography}

\bibliography{report}

\end{document}